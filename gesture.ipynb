{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human Computer Interaction CS449 – CS549\n",
    "\n",
    "Assignment 5: Development of Gesture-based interaction using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/08 01:35:11.222421][info][1815152][Context.cpp:69] Context created with config: default config!\n",
      "[12/08 01:35:11.222440][info][1815152][Context.cpp:74] Context work_dir=/Users/ulasyildiz/Documents/Gesture-Based Interaction/CS449_Gesture_Intereaction\n",
      "[12/08 01:35:11.222442][info][1815152][Context.cpp:77] \t- SDK version: 1.9.4\n",
      "[12/08 01:35:11.222443][info][1815152][Context.cpp:78] \t- SDK stage version: main\n",
      "[12/08 01:35:11.222445][info][1815152][Context.cpp:82] get config EnumerateNetDevice:false\n",
      "[12/08 01:35:11.222447][info][1815152][MacPal.cpp:36] createObPal: create MacPal!\n",
      "[12/08 01:35:11.222793][info][1815152][MacPal.cpp:104] Create PollingDeviceWatcher!\n",
      "[12/08 01:35:11.222800][info][1815152][DeviceManager.cpp:15] Current found device(s): (0)\n",
      "[12/08 01:35:11.222802][info][1815152][Pipeline.cpp:15] Try to create pipeline with default device.\n",
      "[12/08 01:35:11.222803][warning][1815152][ObException.cpp:5] No device found, fail to create pipeline!\n",
      "[12/08 01:35:11.223163][info][1815152][Context.cpp:90] Context destroyed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: not authorized to capture video (status 0), requesting...\n",
      "OpenCV: camera failed to properly initialize!\n",
      "[ WARN:0@16.510] global cap.cpp:323 open VIDEOIO(OBSENSOR): raised unknown C++ exception!\n",
      "\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733610911.301855 1815152 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733610911.304772 1815615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733610911.307410 1815615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe için gerekli modülleri başlat\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Kameradan video akışını başlat\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,  # Algılama güven seviyesi\n",
    "        min_tracking_confidence=0.5   # İzleme güven seviyesi\n",
    ") as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Kamera bulunamadı!\")\n",
    "            break\n",
    "\n",
    "        # Kameradan alınan görüntüyü RGB formatına dönüştür\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "\n",
    "        # Yüz işaret noktalarını çizin\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION)\n",
    "\n",
    "        # Görüntüyü göster\n",
    "        cv2.imshow(\"Yüz Jest Algılama\", frame)\n",
    "\n",
    "        # Çıkmak için 'q' tuşuna basın\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_eye_aspect_ratio(eye_landmarks):\n",
    "    # Göz noktalarını kullanarak EAR hesapla\n",
    "    A = np.linalg.norm(np.array(eye_landmarks[1]) - np.array(eye_landmarks[5]))\n",
    "    B = np.linalg.norm(np.array(eye_landmarks[2]) - np.array(eye_landmarks[4]))\n",
    "    C = np.linalg.norm(np.array(eye_landmarks[0]) - np.array(eye_landmarks[3]))\n",
    "    EAR = (A + B) / (2.0 * C)\n",
    "    return EAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sağ ve sol göz için işaret noktalarını belirle\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "def get_eye_landmarks(face_landmarks, eye_indices):\n",
    "    return [[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] for i in eye_indices]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            left_eye = get_eye_landmarks(face_landmarks, LEFT_EYE)\n",
    "            right_eye = get_eye_landmarks(face_landmarks, RIGHT_EYE)\n",
    "\n",
    "            # Sol ve sağ göz için EAR hesapla\n",
    "            left_ear = calculate_eye_aspect_ratio(left_eye)\n",
    "            right_ear = calculate_eye_aspect_ratio(right_eye)\n",
    "\n",
    "            # Göz kırpma tespiti\n",
    "            if left_ear < 0.2:\n",
    "                print(\"Sol göz kırpıldı! Sandalye sola gidiyor.\")\n",
    "            if right_ear < 0.2:\n",
    "                print(\"Sağ göz kırpıldı! Sandalye sağa gidiyor.\")\n",
    "\n",
    "    cv2.imshow(\"Yüz Jest Algılama\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
